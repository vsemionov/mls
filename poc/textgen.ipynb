{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import math\n",
    "import os\n",
    "import collections\n",
    "import multiprocessing\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchtext\n",
    "import datasets\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4829984f56155b48",
   "metadata": {},
   "source": [
    "splits = {\n",
    "    'train': 0.1,\n",
    "    'val': 0.05,\n",
    "    'test': 0.05\n",
    "}\n",
    "\n",
    "vocab_len = 20_000\n",
    "\n",
    "batch_size = 64\n",
    "seq_len = 128\n",
    "emb_dim = 256\n",
    "\n",
    "data_step = seq_len / 2\n",
    "\n",
    "n_blocks = 4\n",
    "n_heads = 4\n",
    "d_ff = 512\n",
    "dropout = 0.1\n",
    "\n",
    "lr = 3e-4\n",
    "epochs = 20\n",
    "patience = 5\n",
    "lr_factor = 0.1\n",
    "lr_patience = 2\n",
    "min_lr = 1e-5"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6da6662a8d278a1",
   "metadata": {},
   "source": [
    "multiprocessing.set_start_method('fork')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361b5002d87ca9a8",
   "metadata": {},
   "source": [
    "dataset = datasets.load_dataset('wikipedia', '20220301.simple', trust_remote_code=True)\n",
    "dataset"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39843984e155ba3",
   "metadata": {},
   "source": [
    "tokenizer = torchtext.data.utils.get_tokenizer('basic_english')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4409d8e872d39e1",
   "metadata": {},
   "source": [
    "def _tokenize(sample):\n",
    "    sample['tokens'] = tokenizer(sample['text'])\n",
    "    return sample\n",
    "\n",
    "\n",
    "dataset = dataset.map(_tokenize, remove_columns=['text'], num_proc=os.cpu_count())\n",
    "dataset"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2fdd166a334e7e",
   "metadata": {},
   "source": [
    "train_set = dataset['train']\n",
    "train_set, test_set = train_set.train_test_split(int(splits['test'] * len(train_set)), seed=0).values()\n",
    "train_set, val_set = train_set.train_test_split(int(splits['val'] * len(train_set)), seed=0).values()\n",
    "_, train_set = train_set.train_test_split(int(splits['train'] * len(train_set)), seed=0).values()\n",
    "data_splits = {\n",
    "    'train': train_set,\n",
    "    'val': val_set,\n",
    "    'test': test_set\n",
    "}\n",
    "len(train_set), len(val_set), len(test_set)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148113de589d8a53",
   "metadata": {},
   "source": [
    "def build_sample_map(dataset, seq_len):\n",
    "    map = []\n",
    "    for idx, sample in enumerate(tqdm(dataset)):\n",
    "        tokens = sample['tokens']\n",
    "        n_samples = len(tokens) - (seq_len + 1)\n",
    "        if n_samples < 1:\n",
    "            continue\n",
    "        map.extend([(idx, i) for i in range(n_samples)])\n",
    "    return map\n",
    "\n",
    "\n",
    "sample_maps = {}\n",
    "for split in splits:\n",
    "    filename = f'sample_map-{split}-{seq_len}.pt'\n",
    "    try:\n",
    "        sample_map = torch.load(filename)\n",
    "    except FileNotFoundError:\n",
    "        sample_map = build_sample_map(data_splits[split], seq_len=seq_len)\n",
    "        sample_map = torch.tensor(sample_map)\n",
    "        torch.save(sample_map, filename)\n",
    "    sample_maps[split] = sample_map\n",
    "[len(sample_maps[split]) for split in splits]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cd5d5ce6a48501",
   "metadata": {},
   "source": [
    "filename = f'vocab-{vocab_len}.pt'\n",
    "try:\n",
    "    vocab = torch.load(filename)\n",
    "except FileNotFoundError:\n",
    "    vocab = torchtext.vocab.build_vocab_from_iterator((sample['tokens'] for sample in tqdm(data_splits['train'])), specials=['<pad>', '<unk>'], max_tokens=vocab_len)\n",
    "    vocab.set_default_index(vocab['<unk>'])\n",
    "    torch.save(vocab, filename)\n",
    "len(vocab)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994d8d5f44d0806f",
   "metadata": {},
   "source": [
    "class SequenceDataset(data.Dataset):\n",
    "    def __init__(self, dataset, seq_len, tokenizer, vocab, sample_map, cache=False):\n",
    "        super().__init__()\n",
    "        self.dataset = dataset\n",
    "        self.seq_len = seq_len\n",
    "        self.tokenizer = tokenizer\n",
    "        self.vocab = vocab\n",
    "        self.sample_map = sample_map\n",
    "        self.len = len(sample_map)\n",
    "        self.cache = None\n",
    "        if cache:\n",
    "            self.cache = dataset['tokens']\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        idx = int(idx)  # sample dataset passes tensor indices\n",
    "        ds_idx, start_idx = self.sample_map[idx]\n",
    "        ds_idx, start_idx = int(ds_idx), int(start_idx)  # sample map is a tensor\n",
    "        tokens = self.dataset[ds_idx]['tokens'] if self.cache is None else self.cache[ds_idx]\n",
    "        tokens = tokens[start_idx:start_idx + self.seq_len + 1]\n",
    "        indices = self.vocab.lookup_indices(tokens)\n",
    "        indices = torch.tensor(indices)\n",
    "        x, y = indices[:-1], indices[1:]\n",
    "        return x, y\n",
    "\n",
    "\n",
    "seq_data = {split: SequenceDataset(data_splits[split], seq_len, tokenizer, vocab, sample_maps[split], cache=True) for split in splits}"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21822c4fb98e43a",
   "metadata": {},
   "source": [
    "train_loader = data.DataLoader(seq_data['train'], batch_size=batch_size, shuffle=True, num_workers=os.cpu_count(), persistent_workers=True, pin_memory=True)\n",
    "val_loader = data.DataLoader(seq_data['val'], batch_size=batch_size, shuffle=False, num_workers=os.cpu_count(), persistent_workers=True, pin_memory=True)\n",
    "test_loader = data.DataLoader(seq_data['test'], batch_size=batch_size, shuffle=False, num_workers=os.cpu_count(), persistent_workers=True, pin_memory=True)\n",
    "\n",
    "train_batches = int(len(seq_data['train']) / (data_step * batch_size))\n",
    "val_batches = int(len(seq_data['val']) / (data_step * batch_size))\n",
    "test_batches = int(len(seq_data['test']) / (data_step * batch_size))\n",
    "train_batches, val_batches, test_batches"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd29588496b5e604",
   "metadata": {},
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, max_len, d_model):\n",
    "        assert d_model % 2 == 0\n",
    "        super().__init__()\n",
    "        pos = torch.arange(max_len).float()\n",
    "        i = torch.arange(d_model // 2)\n",
    "        den = 10_000 ** (2 * i / d_model)\n",
    "        p_i = pos.unsqueeze(1) / den\n",
    "        enc = torch.empty(max_len, d_model)\n",
    "        enc[:, 0::2] = torch.sin(p_i)\n",
    "        enc[:, 1::2] = torch.cos(p_i)\n",
    "        self.register_buffer('enc', enc, persistent=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.enc[:x.size(-2)]\n",
    "\n",
    "\n",
    "class Transformer(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_len, emb_dim)\n",
    "        self.pos_enc = PositionalEncoding(seq_len, emb_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(emb_dim, n_heads, d_ff, dropout=dropout, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, n_blocks)\n",
    "        self.linear = nn.Linear(emb_dim, vocab_len)\n",
    "        mask = nn.Transformer.generate_square_subsequent_mask(seq_len)\n",
    "        self.register_buffer('mask', mask)\n",
    "\n",
    "    def forward(self, x):\n",
    "        n = x.size(1)\n",
    "        x = self.embedding(x) * math.sqrt(self.embedding.embedding_dim)\n",
    "        x = x + self.pos_enc(x).unsqueeze(0)\n",
    "        x = self.transformer(x, mask=self.mask[:n, :n], is_causal=True)\n",
    "        y = self.linear(x)\n",
    "        return y\n",
    "\n",
    "    @staticmethod\n",
    "    def loss(logits, targets):\n",
    "        return F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=lr_factor, patience=lr_patience, min_lr=min_lr)\n",
    "        return {'optimizer': optimizer, 'lr_scheduler': {'scheduler': scheduler, 'monitor': 'val_loss', 'interval': 'epoch'}}\n",
    "\n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        x, _ = batch\n",
    "        y = self.forward(x)\n",
    "        return y\n",
    "\n",
    "    def _dev_step(self, batch, batch_idx, name, **metrics):\n",
    "        _, targets = batch\n",
    "        logits = self.predict_step(batch, batch_idx)\n",
    "        loss = self.loss(logits, targets)\n",
    "        correct = int((logits.detach().argmax(dim=2) == targets).sum())\n",
    "        accuracy = correct / targets.numel()\n",
    "        self.log_dict({f'{name}_loss': loss, f'{name}_acc': accuracy, **metrics}, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self._dev_step(batch, batch_idx, 'train')\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self._dev_step(batch, batch_idx, 'val')\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        return self._dev_step(batch, batch_idx, 'test')\n",
    "\n",
    "\n",
    "model = Transformer()\n",
    "sum(p.numel() for p in model.parameters())"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cd2696de44d6db",
   "metadata": {},
   "source": [
    "callbacks = [\n",
    "    lr_monitor := pl.callbacks.LearningRateMonitor(),\n",
    "    early_stopping := pl.callbacks.EarlyStopping(monitor='val_loss', mode='min', patience=patience),\n",
    "    model_checkpoint := pl.callbacks.ModelCheckpoint(monitor='val_loss', mode='min')\n",
    "]\n",
    "trainer = pl.Trainer(max_epochs=epochs, limit_train_batches=train_batches, limit_val_batches=val_batches, limit_test_batches=test_batches, callbacks=callbacks, logger=CSVLogger(save_dir='./'))\n",
    "trainer.fit(model, train_loader, val_loader)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e8c18d2584c7d1",
   "metadata": {},
   "source": [
    "model = Transformer.load_from_checkpoint(model_checkpoint.best_model_path)\n",
    "model_checkpoint.best_model_path"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c97305",
   "metadata": {},
   "source": [
    "trainer.test(model, test_loader)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e165c7f5ba3822cb",
   "metadata": {},
   "source": [
    "log = pd.read_csv(f'{trainer.logger.log_dir}/metrics.csv')\n",
    "df = log.set_index('epoch')\n",
    "df['val_loss'] = df.val_loss.shift(1)\n",
    "df['val_acc'] = df.val_acc.shift(1)\n",
    "df = df[df['train_loss'].notnull() | df['val_loss'].notnull()]\n",
    "df[['train_loss', 'val_loss']].plot()\n",
    "plt.show()\n",
    "df[['train_acc', 'val_acc']].plot()\n",
    "plt.show()\n",
    "log[log['lr-Adam'].notnull()][['lr-Adam']].plot()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e2cf9a",
   "metadata": {},
   "source": [
    "# model = Transformer.load_from_checkpoint('../models/textgen/epoch=18-step=9728.ckpt')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65057b3279adf30a",
   "metadata": {},
   "source": [
    "@torch.no_grad()\n",
    "def sample(\n",
    "        model,\n",
    "        x,\n",
    "        output_length,\n",
    "        block_size,\n",
    "        eos_class=None,\n",
    "        exclude_classes=None,\n",
    "        temperature=1,\n",
    "        top_k=None,\n",
    "        top_p=None,\n",
    "        generator=None\n",
    "):\n",
    "    model.eval()\n",
    "    seq = x\n",
    "    for _ in range(output_length):\n",
    "        inputs = seq[-block_size:].unsqueeze(0)\n",
    "        logits = model.forward(inputs).squeeze(0)[-1]\n",
    "        if exclude_classes:\n",
    "            logits[exclude_classes] = float('-inf')\n",
    "        logits = logits / temperature\n",
    "        probas = logits.softmax(dim=-1)\n",
    "        if top_k:\n",
    "            probas, indices = probas.topk(top_k)\n",
    "        else:\n",
    "            indices = torch.arange(probas.size(-1))\n",
    "        if top_p:\n",
    "            sorted_probas, sorted_indices = probas.sort()  # ascending sort simplifies the following\n",
    "            cumprobas = sorted_probas.cumsum(-1)\n",
    "            nucleus_size = cumprobas.size(-1) - torch.sum(cumprobas <= (1 - top_p))\n",
    "            nucleus_indices = sorted_indices[-nucleus_size:]\n",
    "            probas = sorted_probas[-nucleus_size:]\n",
    "            indices = indices[nucleus_indices]\n",
    "        index = probas.multinomial(1, generator=generator)\n",
    "        if index == eos_class:\n",
    "            break\n",
    "        seq = torch.cat([seq, indices[index]])\n",
    "    return seq[x.size(-1):]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906a4e27",
   "metadata": {},
   "source": [
    "@torch.no_grad()\n",
    "def beam_search(\n",
    "        model,\n",
    "        x,\n",
    "        beam_width,\n",
    "        output_length,\n",
    "        block_size=None,\n",
    "        eos_class=None,\n",
    "        exclude_classes=None,\n",
    "        length_penalty=0\n",
    "):\n",
    "    Node = collections.namedtuple('Node', ['path', 'proba', 'score'])\n",
    "    model.eval()\n",
    "    empty = torch.tensor([], dtype=torch.int64, device=model.device)\n",
    "    root = Node(empty, 0.0, 0.0)\n",
    "    nodes = branches = [root]\n",
    "    leaves = []\n",
    "    for level in range(output_length):\n",
    "        candidates = []\n",
    "        score_divisor = (level + 1) ** length_penalty\n",
    "        best_score = max(leaf.score for leaf in leaves) if leaves else float('-inf')\n",
    "        early_stopping_divisor = score_divisor if length_penalty <= 0 else output_length ** length_penalty\n",
    "        for branch in branches:\n",
    "            if branch.proba / early_stopping_divisor < best_score:\n",
    "                continue\n",
    "            _x, _path = x, branch.path\n",
    "            if block_size:\n",
    "                _path = branch.path[-block_size:]\n",
    "                _x = x[max(x.size(0) + _path.size(0) - block_size, 0):]\n",
    "            inputs = torch.cat([_x, _path]).unsqueeze(0)\n",
    "            logits = model(inputs).squeeze(0)[-1]\n",
    "            if exclude_classes:\n",
    "                logits[exclude_classes] = float('-inf')\n",
    "            probas = logits.log_softmax(0)\n",
    "            probas, indices = probas.topk(beam_width)\n",
    "            probas += branch.proba\n",
    "            scores = probas / score_divisor\n",
    "            cand = [Node(torch.cat([branch.path, indices[i:i+1]]), proba, score)\n",
    "                    for i, (proba, score) in enumerate(zip(probas, scores))]\n",
    "            candidates.extend(cand)\n",
    "        candidates += leaves\n",
    "        candidates = sorted(candidates, key=lambda node: node.score, reverse=True)\n",
    "        nodes = candidates[:beam_width]\n",
    "        leaves = [node for node in nodes if node.path[-1] == eos_class]\n",
    "        branches = set(nodes) - set(leaves)\n",
    "        if not branches:\n",
    "            break\n",
    "    output = max(nodes, key=lambda node: (node.path[-1] == eos_class, node.score))\n",
    "    if output[-1] == eos_class:\n",
    "        output = output[:-1]\n",
    "    return output"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ecd8be",
   "metadata": {},
   "source": [
    "prompt = 'marry had a little lamb'\n",
    "indices = torch.tensor(vocab.lookup_indices(tokenizer(prompt)), device=model.device)\n",
    "exclude = vocab.lookup_indices(['<pad>', '<unk>'])\n",
    "indices = sample(model, indices, 100, block_size=seq_len, exclude_classes=exclude, temperature=1, top_k=None, top_p=None)\n",
    "tokens = vocab.lookup_tokens(indices.tolist())\n",
    "prompt + ' ' + ' '.join(tokens)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79afa743",
   "metadata": {},
   "source": [
    "prompt = 'marry had a little lamb'\n",
    "indices = torch.tensor(vocab.lookup_indices(tokenizer(prompt)), device=model.device)\n",
    "exclude = vocab.lookup_indices(['<pad>', '<unk>'])\n",
    "indices = beam_search(model, indices, 10, 100, block_size=seq_len, exclude_classes=exclude)\n",
    "tokens = vocab.lookup_tokens(indices.tolist())\n",
    "prompt + ' ' + ' '.join(tokens)"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
